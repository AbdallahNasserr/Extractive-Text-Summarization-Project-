{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport os\nimport nltk\nimport re\nimport math\nimport operator\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk import pos_tag                   # ---------> POS tagging\nfrom nltk.corpus import stopwords , state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\nimport spacy\nfrom spacy import displacy                 # ---------> for Visualization\nfrom string import punctuation\nfrom string import punctuation\nfrom IPython.display import HTML\nfrom scipy.stats import norm\n# ---------> for highlighting words\n# Downloading Files\n#nltk.download('stopwords')\n#nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnlp = spacy.load('en_core_web_sm')\nwordlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\nnltk.download('stopwords')\nnltk.download('punkt')\nStopwords = set(stopwords.words('english'))\nimport streamlit as st\nimport uuid\n\n# Define the title text\ntitle_text = \"<h1 style='text-align: center; color: white;'>Text Summarization Web App</h1>\"\n\n# Define the CSS for the gradient background\ngradient_bg_css = \"\"\"\n    background: linear-gradient(to right, #4C0FB5, #198DD0); \n    padding: 20px; \n    border-radius: 10px; \n    border: 4px solid white; /* Adding a 2px solid white border */\n\"\"\"\n\n# Combine title text with gradient background CSS\nstyled_title = f\"<div style='{gradient_bg_css}'>{title_text}</div>\"\n\n# Render the title\nst.write(\"\")\nst.markdown(styled_title, unsafe_allow_html=True)\nst.write(\"\")\nst.write(\"\")\n\n# Initialize text1 variable\ntext1 = \"\"\n\n# Function to handle actions when \"Upload .txt/.pdf file\" button is clicked\ndef upload_file():\n    global text1  # Use the global variable text1\n    uploaded_file = st.file_uploader(\"Upload .txt/.pdf file\", type=['txt', 'pdf'])\n    if uploaded_file is not None:\n        # Process the uploaded file\n        text = uploaded_file.read().decode('utf-8')  # Read the contents of the file\n        st.text_area(\"Text from file\", value=text, height=200)\n        #st.button(\"Submit\", key=str(uuid.uuid4()))  # Unique key for each button\n        text1 = text  # Update text1 with the uploaded text\n\n# Function to handle actions when \"Enter Manually\" button is clicked\ndef enter_manually():\n    global text1  # Use the global variable text1\n    text = st.text_area(\"Enter Text Manually\", height=200)\n    text1 = text  # Update text1 with the manually entered text\n    #st.button(\"Submit\", key=\"submit_button_manual\")  # Fixed key for manual submission\n    # Further processing code here if needed\n\n# Main part of the app\nst.title(\"Data Input Options\")\nst.write(\"\")\n\n# Button to select data input method\ninput_method = st.radio(\"Select Data Input Method\", (\"Upload data\", \"Enter Manually\"))\n\n# Show appropriate input widgets based on the selected input method\nif input_method == \"Upload data\":\n    st.subheader(\"Upload Data\")\n    upload_file()\n\nelif input_method == \"Enter Manually\":\n    st.subheader(\"Enter Data Manually\")\n    enter_manually()\n\n# Display the text1 variable after input\n#st.write(\"Text1:\", text1)\nst.write(\"\")\nst.write(\"\")\nuser_input = st.number_input(\"Enter Percentage for TF-IDF Summary:\")\ntext  = text1 \n# Tokenization using NLTK library\nwords = word_tokenize(text)\nvis = words\n#st.write(vis[:10])\n# Sentence Tokenization\ni = 1\nsentences = sent_tokenize(text)\ndef lowercasing(word):\n        word = word.lower()\nfor word in words:\n    lowercasing(word)\nvis = words\n\nstopWords = list(stopwords.words(\"english\"))+list(punctuation)+list([0,1,2,3,4,5,6,7,8,9])\n\nwithout_stop = []\ndef stopword_removal(word):\n    if word not in stopWords:\n        without_stop.append(word)\nfor word in words:\n    stopword_removal(word)\nwords = without_stop\nvis = words\nvis2 = list(set(vis))\n#st.write(vis[:2])\nstemmed_words = []\nfor word in words:\n    stemmed = stemmer.stem(word)\n    stemmed_words.append(stemmed)\nwords = stemmed_words\n#words[:10]\nlemmatized_words = []\nfor word in words:\n    lemmatized_words.append(wordlemmatizer.lemmatize(word))\nwords = lemmatized_words\n#words[:10]\ndef remove_special_characters(text):\n    regex = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(regex,'',text)\n    return text\ndef freq(words):\n    words = [word.lower() for word in words]\n    freqTable = {}\n    words_unique = []\n    for word in words:\n        if word not in words_unique:\n            words_unique.append(word)\n    for word in words_unique:\n        freqTable[word] = words.count(word)\n    return freqTable\n\n\ndef idf_score(no_of_sentences,word,sentences):\n    no_of_sentence_containing_word = 0\n    for sentence in sentences:\n        sentence = remove_special_characters(str(sentence))\n        sentence = re.sub(r'\\d+', '', sentence)\n        sentence = sentence.split()\n        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n        sentence = [word.lower() for word in sentence]\n        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n        if word in sentence:\n            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n    return idf\n\ndef tf_score(word,sentence):\n    freq_sum = 0\n    word_frequency_in_sentence = 0\n    len_sentence = len(sentence)\n    for word_in_sentence in sentence.split():\n        if word == word_in_sentence:\n            word_frequency_in_sentence = word_frequency_in_sentence + 1\n    tf =  word_frequency_in_sentence/ len_sentence\n    return tf\n\n\ndef pos_tagging(text):\n    pos_tag = nltk.pos_tag(text.split())\n    pos_tagged_noun_verb = []\n    for word,tag in pos_tag:\n        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n            pos_tagged_noun_verb.append(word)\n    return pos_tagged_noun_verb\n\n\n\ndef tf_idf_score(tf,idf):\n    return tf*idf\n\ndef word_tfidf(freqTable,word,sentences,sentence):\n    word_tfidf = []\n    tf = tf_score(word,sentence)\n    idf = idf_score(len(sentences),word,sentences)\n    tf_idf = tf_idf_score(tf,idf)\n    return tf_idf\n\ndef sentence_importance(sentence,freqTable,sentences):\n    sentence_score = 0\n    sentence = remove_special_characters(str(sentence))\n    sentence = re.sub(r'\\d+', '', sentence)\n    pos_tagged_sentence = []\n    no_of_sentences = len(sentences)\n    pos_tagged_sentence = pos_tagging(sentence)\n    for word in pos_tagged_sentence:\n        if word.lower() not in Stopwords and word not in Stopwords and len(word)>1:\n            word = word.lower()\n            word = wordlemmatizer.lemmatize(word)\n            sentence_score = sentence_score + word_tfidf(freqTable,word,sentences,sentence)\n    return sentence_score\nfreqTable={}\nfor word in words:\n    word = word.lower()\n    if word not in stopWords:\n        if word in freqTable:\n            freqTable[word] += 1\n        else:\n            freqTable[word] = 1\nword_freq = freqTable\n#st.write(len(sentences))\n# Take text input from the user\n# Display the input\n#st.write(\"You entered:\", user_input)\n\n\nimport opendatasets as od\nod.download(r'https://www.kaggle.com/datasets/sawarn69/glove6b100dtxt?select=glove.6B.100d.txt')\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = r\"C:\\Users\\acer\\Downloads\\glove6b100dtxt\\glove.6B.100d.txt\"\n#glove_input_file = r\"/content/glove6b100dtxt/glove.6B.100d.txt\"\n# Extract word vectors\nword_embeddings = {}\nf = open(glove_input_file, encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()\nsentence_vectors = []\nfor i in sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)\n\n# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\n#sim_mat.shape\nfrom sklearn.metrics.pairwise import cosine_similarity\ni=0\nj=1\n#cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0][0]\n\n\n\n\nfor i in range(len(sentences)):\n    for j in range(len(sentences)):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\n#print(sim_mat.shape)\n\nimport networkx as nx\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)\nranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n\n\n\n\ninput_user =user_input #int(input('Percentage of information to retain(in percent):'))\nno_of_sentences = int((input_user * len(sentences))/100)\n#st.write(no_of_sentences)\n#*******************************\nc = 1\nsentence_with_importance = {}\nfor sent in sentences:\n    sentenceimp = sentence_importance(sent,word_freq,sentences)\n    sentence_with_importance[c] = sentenceimp\n    c = c+1\nsentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\ncnt = 0\nsummary = []\nsentence_no = []\nfor word_prob in sentence_with_importance:\n    if cnt < no_of_sentences:\n        sentence_no.append(word_prob[0])\n        cnt = cnt+1\n    else:\n        break\nsentence_no.sort()\ncnt = 1\nfor sentence in sentences:\n    if cnt in sentence_no:\n        summary.append(sentence)\n        summary.append('\\n')\n    cnt = cnt+1\n\nsummary = \" \".join(summary)\n#print(\"\\n\")\nst.write(\"\")\nst.write(\"\")\nsn = int(st.number_input(\"Please Enter Number of Sentences for Word Embedding Summary: \"))\n# Define the subheader text\nsubheader_text = \"TF-IDF Approach Summary:\"\nsubheader_text2 = \"Word Embedding Approach Summary:\"\n\n# Define the CSS for the gradient background\ngradient_bg_css = \"\"\"\n    background: linear-gradient(to right, #4C0FB5, #198DD0); \n    padding: 10px; \n    border-radius: 10px; \n    color: white;\n\"\"\"\n\n# Combine subheader text with gradient background CSS\nstyled_subheader = f\"<div style='{gradient_bg_css}'>{subheader_text}</div>\"\nstyled_subheader2 = f\"<div style='{gradient_bg_css}'>{subheader_text2}</div>\"\n\n# Render the subheader\nst.markdown(styled_subheader, unsafe_allow_html=True)\nst.write(\"\")\nst.write(\"\")\nst.write(summary)\n\n# Specify number of sentences to form the summary\nst.write(\"\")\nst.write(\"\")\nst.write(\"\")\nst.write(\"\")\n# Generate summary\nst.markdown(styled_subheader2, unsafe_allow_html=True)\nresult = \"\"\n# Generate summary\nfor i in range(sn):\n    result += (ranked_sentences[i][1] + \" \")\n    \nst.write(result)","metadata":{"_uuid":"3b435fd3-15d5-4f13-8260-45701c8d5406","_cell_guid":"95932862-1c15-468f-bf4d-6232383ce5a2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}